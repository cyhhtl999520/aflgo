\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{float}  % For better figure placement control

\modulolinenumbers[5]

\journal{Journal of Systems and Software}

\bibliographystyle{elsarticle-num}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  language=C,
  numbers=left,
  numberstyle=\tiny,
  showstringspaces=false,
  captionpos=b
}

\begin{document}

\begin{frontmatter}

\title{Variable State Diversity-Guided Directed Fuzzing}

\author[inst1]{Anonymous Author 1}
\author[inst1]{Anonymous Author 2}
\author[inst2]{Anonymous Author 3}

\address[inst1]{Department of Computer Science, University A, City, Country}
\address[inst2]{School of Software Engineering, University B, City, Country}

\begin{abstract}
Directed greybox fuzzing has become a prominent technique for targeted vulnerability detection by focusing computational resources on specific code locations. However, existing approaches primarily rely on control-flow information while neglecting the critical role of program data states in triggering vulnerabilities. Many security bugs require specific variable value combinations to manifest, which purely coverage-driven approaches may fail to expose efficiently. This paper presents GFuzz, a novel variable state diversity-guided directed fuzzing method that integrates fine-grained data-state awareness into the fuzzing process. GFuzz employs multi-strategy variable identification to locate key variables related to target locations, instruments programs to monitor runtime states with type-specific handling, and uses state diversity as additional feedback for seed selection and energy allocation. An adaptive weight adjustment mechanism dynamically balances coverage-based and state-based guidance throughout the campaign. We implemented GFuzz as an extension of AFLGo and evaluated it on four real-world programs containing known vulnerabilities. Experimental results demonstrate significant improvements over AFLGo with 17.7\% increase in path discovery, 12.0\% improvement in code coverage, and 26.0\% enhancement in crash detection, while maintaining acceptable 17.9\% runtime overhead.
\end{abstract}

\begin{keyword}
Directed fuzzing \sep Greybox fuzzing \sep Variable state diversity \sep Vulnerability detection \sep Software testing
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
\label{sec:introduction}

Software vulnerabilities represent one of the most critical challenges in modern computing, with thousands of new security flaws discovered annually across diverse software ecosystems. The economic impact of these vulnerabilities is substantial, with data breaches and security incidents costing organizations billions of dollars each year. Traditional software testing approaches often struggle to efficiently detect these vulnerabilities, particularly those that require specific execution conditions to trigger. Fuzzing has emerged as a highly effective automated technique for vulnerability detection, successfully identifying numerous critical security flaws in widely-deployed software systems ranging from web browsers to operating system kernels.

Among the various fuzzing techniques that have been developed, greybox fuzzing strikes an effective balance between the efficiency of blackbox approaches and the precision of whitebox methods. American Fuzzy Lop (AFL) pioneered the coverage-guided greybox fuzzing paradigm, which uses lightweight instrumentation to track program execution paths and evolves test inputs through genetic algorithms. The success of AFL has inspired a rich ecosystem of extensions and variants that improve different aspects of the fuzzing process, including power scheduling strategies, mutation operators, and seed selection heuristics.

Directed greybox fuzzing represents a specialized branch of fuzzing that focuses testing efforts on specific code locations of interest rather than attempting to maximize overall code coverage. This targeted approach is particularly valuable in scenarios such as patch testing, where developers need to verify that recent code changes have not introduced new vulnerabilities, security audits focused on critical system components, and reproduction of crashes reported from the field. AFLGo introduced the foundational concept of directed fuzzing by computing static distances from program locations to specified targets and using these distances to guide the fuzzing process toward reaching the target code.

However, despite the effectiveness of existing directed fuzzing approaches, they share a fundamental limitation in that they primarily rely on control-flow information to guide the fuzzing process. These methods calculate distances based on call graphs and control flow graphs, then preferentially select test inputs that exercise execution paths closer to the target locations. While this control-flow-centric guidance effectively helps the fuzzer reach target code locations, it neglects a critical dimension of the problem: many vulnerabilities require not only reaching specific code locations but also satisfying particular data conditions involving program variables. For example, buffer overflow vulnerabilities typically manifest only when size-related variables exceed certain thresholds, use-after-free bugs require specific pointer states, and integer overflow vulnerabilities depend on particular arithmetic operations reaching boundary values.

To illustrate this limitation concretely, consider a vulnerability in an XML parsing library where a heap buffer overflow occurs during element processing. The vulnerable code path requires that the nesting depth of XML elements exceed a predetermined threshold while simultaneously the allocated buffer size for element data falls below the required capacity. A purely coverage-driven fuzzer might generate thousands of inputs that successfully reach the vulnerable parsing function, yet fail to trigger the actual vulnerability because the generated XML documents do not simultaneously satisfy both the depth condition and the buffer size condition. The fuzzer lacks awareness of which combinations of variable states have been explored and which critical state space regions remain unexplored, leading to inefficient and potentially ineffective testing of data-dependent vulnerabilities.

This paper presents GFuzz, a novel directed fuzzing approach that addresses the limitation of existing methods by systematically incorporating variable state diversity into the fuzzing process. Our key insight is that by identifying variables that are relevant to target locations, monitoring their runtime states during execution, and using the diversity of observed variable states as additional feedback for seed selection and energy allocation, we can significantly improve the effectiveness of directed fuzzing for detecting vulnerabilities that require specific data conditions. GFuzz extends the traditional control-flow-based guidance of directed fuzzing with a complementary data-state dimension, enabling more thorough exploration of the program state space around target locations.

The paper structure proceeds as follows. Section~\ref{sec:problem} articulates the problem and motivation in detail through concrete examples. Section~\ref{sec:approach} presents our approach including the algorithms for key variable identification, state monitoring, diversity evaluation, and adaptive scheduling. Section~\ref{sec:empirical} describes our comprehensive empirical evaluation on four real-world programs. Section~\ref{sec:discussion} discusses the implications, limitations, and threats to validity. Section~\ref{sec:related} surveys related work in fuzzing and program analysis. Section~\ref{sec:conclusion} concludes with a summary of contributions and future research directions.

\section{Problem and Motivation}
\label{sec:problem}

\subsection{Limitations of Control-Flow-Centric Directed Fuzzing}

Existing directed greybox fuzzing techniques have demonstrated considerable success in reaching specific target locations within programs by leveraging control-flow information. AFLGo, the seminal work in this area, computes static distances from all program basic blocks to target locations using both function-level call graph distances and basic block-level control flow graph distances. During fuzzing, seeds that execute basic blocks closer to targets receive higher priority for mutation and more energy allocation. This approach effectively guides the fuzzer toward target code by preferentially exploring execution paths that make progress toward the targets.

Despite this effectiveness, the control-flow-centric approach possesses an inherent blind spot regarding program data states. Consider the simplified code example shown in Listing~\ref{lst:motivating}. This code represents a pattern commonly found in parsing and data processing applications where vulnerabilities depend on specific relationships between program variables.

\begin{lstlisting}[caption={Motivating example: data-dependent vulnerability},label=lst:motivating,float=t]
void process_data(char *input, int len) {
    int depth = 0;
    int buffer_size = 1024;
    char *buffer = malloc(buffer_size);
    
    for (int i = 0; i < len; i++) {
        if (input[i] == '{') {
            depth++;
            if (depth > MAX_DEPTH) {
                // Target location: potential vulnerability
                if (buffer_size < depth * ELEMENT_SIZE) {
                    // Vulnerability triggered
                    memcpy(buffer, input, depth * ELEMENT_SIZE);
                }
            }
        }
    }
}
\end{lstlisting}

In this example, the vulnerability at the target location requires that two conditions be satisfied simultaneously: the depth variable must exceed the MAX\_DEPTH threshold, and the buffer\_size variable must be insufficient for the required memory operation. A control-flow-guided fuzzer that successfully reaches the target location through mutations that increase the depth value may still fail to trigger the actual vulnerability if the relationship between depth and buffer\_size does not meet the required condition. Without awareness of which combinations of depth and buffer\_size values have been tested, the fuzzer essentially performs blind exploration of the data state space, leading to redundant testing of similar variable states while missing critical corner cases.

\subsection{The Data State Exploration Challenge}

The challenge of efficiently exploring the data state space around target locations is compounded by several factors. First, the space of possible variable states is typically enormous, even for programs with a moderate number of variables. For a program with n integer variables, each potentially taking values from a 32-bit range, the theoretical state space contains $2^{32n}$ possible states. While many of these states may be unreachable in practice due to program logic constraints, the reachable state space still represents a vast search space that requires intelligent exploration strategies.

Second, not all variables contribute equally to the likelihood of triggering vulnerabilities. Many program variables are peripheral to the core functionality around target locations and their states have minimal impact on vulnerability triggering. For instance, loop counter variables, temporary storage variables, and formatting flags typically do not directly influence memory safety or security-critical operations. Attempting to track and diversify the states of all program variables would introduce prohibitive performance overhead and dilute the effectiveness of state-based guidance with noise from irrelevant variables.

Third, different types of variables require different treatment in terms of state encoding, similarity comparison, and diversity evaluation. Numeric variables can be compared based on value equality or binning strategies, string variables require string similarity metrics like edit distance, and pointer variables need address-based comparison with consideration for relative positioning. A uniform treatment of all variable types would fail to capture the nuanced relationships between variable states that are meaningful for vulnerability detection.

Figure~\ref{fig:comparison} illustrates the conceptual difference between traditional control-flow-guided directed fuzzing and our state-diversity-guided approach. Traditional approaches focus on reaching target locations through control flow paths, represented by the progression from entry point to target along program paths. Our approach adds a complementary dimension by systematically exploring diverse variable states at and around the target locations, increasing the likelihood of satisfying the data conditions required to trigger vulnerabilities.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\columnwidth]{figures/approach_comparison.pdf}
\caption{Comparison between traditional control-flow-guided directed fuzzing (left) and GFuzz's state-diversity-guided approach (right). Traditional approaches focus on reaching targets through control flow, while GFuzz additionally explores diverse variable states.}
\label{fig:comparison}
\end{figure}

\subsection{Motivating Requirements}

Based on these observations, we identify three key requirements for effectively incorporating variable state diversity into directed fuzzing. First, the approach must systematically identify which variables are most relevant to target locations, focusing monitoring and diversification efforts on variables that are likely to influence vulnerability triggering while filtering out peripheral variables that add noise and overhead. Second, the state monitoring mechanism must efficiently track variable values during execution with type-appropriate encoding that captures meaningful differences between states without introducing excessive performance overhead that would undermine fuzzing throughput. Third, the diversity evaluation and seed scheduling mechanisms must effectively leverage state information to guide the fuzzer toward unexplored regions of the data state space while maintaining balance with traditional coverage-based guidance to ensure overall exploration effectiveness.

These requirements inform the design of GFuzz, which we present in the following section. Our approach addresses each requirement through carefully designed components that work together to enhance directed fuzzing with data-state awareness while maintaining the efficiency and practicality necessary for real-world application.

\section{Approach}
\label{sec:approach}

GFuzz extends directed greybox fuzzing with systematic variable state diversity guidance through four tightly integrated components. Figure~\ref{fig:architecture} presents the overall architecture, showing how these components interact during the fuzzing workflow. The workflow begins with offline preprocessing to identify key variables and compute distances, continues with instrumented program execution to monitor variable states, and uses runtime diversity evaluation to guide seed selection and energy allocation throughout the fuzzing campaign.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\columnwidth]{figures/gfuzz_architecture.pdf}
\caption{GFuzz architecture showing the four main components and their interactions during preprocessing and fuzzing phases.}
\label{fig:architecture}
\end{figure}

\subsection{Key Variable Identification}

The first component identifies which program variables should be monitored for state diversity. Tracking all variables in a program would be impractical due to performance overhead and would dilute the effectiveness of state-based guidance with information from irrelevant variables. GFuzz employs a multi-strategy filtering approach that combines three complementary criteria to identify key variables that are most likely to influence vulnerability triggering around target locations.

The distance-based filtering strategy leverages the intuition that variables in functions close to target locations are more likely to be relevant than variables in distant functions. We compute the call graph distance from each function to functions containing target locations, then select variables from functions within a configurable distance threshold. Through empirical evaluation across multiple programs, we determined that a threshold of three hops provides an effective balance between capturing relevant variables and limiting the scope of monitoring. This strategy ensures that we focus on variables that are computationally reachable from targets without casting an overly broad net.

The memory-safety-related filtering strategy recognizes that many vulnerabilities involve memory operations such as buffer accesses, pointer dereferences, and memory allocation. We therefore prioritize variables that appear in memory-related contexts including array indexing expressions, pointer arithmetic operations, arguments to memory allocation and deallocation functions, buffer size calculations, and loop bounds that control memory access patterns. Variables identified through this strategy are particularly likely to be relevant for common vulnerability classes such as buffer overflows, use-after-free, and heap corruption.

The semantic type filtering strategy applies type-based heuristics to focus on variable categories that commonly appear in vulnerability conditions. Integer variables often control array indices, buffer sizes, and loop bounds, making them critical for memory safety. Pointer variables are central to many memory corruption vulnerabilities. String variables appear frequently in parsing and data processing contexts where vulnerabilities occur. We exclude floating-point variables unless they appear in security-critical comparisons, and we filter most structure members except for size fields and pointers, as these tend to be less directly involved in vulnerability triggering.

Algorithm~\ref{alg:key-vars} formalizes this multi-strategy identification process. The algorithm takes as input the program call graph, control flow graphs, target location specifications, and the distance threshold, then produces the set of key variables by iteratively applying the three filtering strategies. The output includes metadata for each variable such as its type, the function containing it, and its role in memory operations, which informs subsequent monitoring and diversity evaluation.

\begin{algorithm}[t]
\caption{Multi-Strategy Key Variable Identification}
\label{alg:key-vars}
\begin{algorithmic}[1]
\REQUIRE Program $P$, Targets $T$, Distance threshold $h$
\ENSURE Key variables $V_K$ with metadata
\STATE $CG \leftarrow$ ConstructCallGraph($P$)
\STATE $F_T \leftarrow$ \{$f$ $|$ $f$ contains target in $T$\}
\STATE $F_D \leftarrow$ \{$f$ $|$ $\exists f_t \in F_T$: $d_{CG}(f, f_t) \leq h$\}
\STATE $V_K \leftarrow \emptyset$
\FOR{each function $f \in F_D$}
    \STATE $CFG_f \leftarrow$ GetControlFlowGraph($f$)
    \FOR{each variable $v$ declared or used in $f$}
        \STATE $pass\_type \leftarrow$ CheckTypeFilter($v$)
        \STATE $pass\_memory \leftarrow$ CheckMemoryContext($v$, $CFG_f$)
        \IF{$pass\_type$ AND $pass\_memory$}
            \STATE $metadata \leftarrow$ ExtractMetadata($v$, $f$, $CFG_f$)
            \STATE $V_K \leftarrow V_K \cup \{(v, metadata)\}$
        \ENDIF
    \ENDFOR
\ENDFOR
\RETURN $V_K$
\end{algorithmic}
\end{algorithm}

\subsection{Variable State Monitoring}

Once key variables are identified, GFuzz instruments the target program to monitor their runtime states during execution. The instrumentation must be carefully designed to minimize performance impact while capturing sufficient information to enable effective diversity evaluation. We implement instrumentation at the LLVM intermediate representation level, inserting monitoring calls after instructions that define or modify key variables.

The state encoding strategy differs based on variable type to appropriately capture meaningful state information. For integer variables, we directly record the numeric value with normalization to a fixed 32-bit representation. Large integer types use a fast hash function to map values to the fixed size while preserving value diversity. Character variables store their ASCII codes directly. For string variables, we compute a hash of the string content using MurmurHash3, a fast non-cryptographic hash function that provides good distribution properties and collision resistance. Pointer variables record the address modulo a large prime number, which captures relative positioning information while maintaining a compact representation and privacy by not exposing absolute addresses.

The instrumentation uses a shared memory region to communicate state information efficiently between the instrumented target program and the fuzzer. Each key variable is assigned a fixed offset in the shared memory map where its current state value is stored. When a variable is modified during execution, the instrumentation code computes the appropriate encoded representation and writes it atomically to the corresponding offset. The fuzzer reads this shared memory region after each test case execution to obtain the complete state vector observed during that execution. This design minimizes overhead compared to alternative approaches such as file-based communication or interprocess messaging.

Algorithm~\ref{alg:monitoring} describes the state monitoring process that occurs during program execution. For each modification to a key variable during execution, the monitoring code computes the type-appropriate state encoding and records it in the shared memory map. The algorithm ensures that we capture the most recent state of each variable observed during the execution, which represents the final state when execution completes or crashes.

\begin{algorithm}[t]
\caption{Runtime Variable State Monitoring}
\label{alg:monitoring}
\begin{algorithmic}[1]
\REQUIRE Key variables $V_K$, Shared memory map $M$
\ENSURE State vector $S$ for current execution
\STATE $S \leftarrow$ Initialize empty state map
\FOR{each execution event $e$}
    \IF{$e$ modifies variable $v \in V_K$}
        \STATE $value \leftarrow$ GetVariableValue($v$)
        \STATE $type \leftarrow$ GetVariableType($v$)
        \IF{$type$ = INTEGER}
            \STATE $encoded \leftarrow$ NormalizeInteger($value$)
        \ELSIF{$type$ = CHARACTER}
            \STATE $encoded \leftarrow$ GetASCII($value$)
        \ELSIF{$type$ = STRING}
            \STATE $encoded \leftarrow$ MurmurHash3($value$)
        \ELSIF{$type$ = POINTER}
            \STATE $encoded \leftarrow$ $value$ mod PRIME
        \ENDIF
        \STATE $offset \leftarrow$ GetOffset($v$, $M$)
        \STATE WriteAtomic($M$, $offset$, $encoded$)
        \STATE $S[v] \leftarrow encoded$
    \ENDIF
\ENDFOR
\RETURN $S$
\end{algorithmic}
\end{algorithm}

\subsection{State Diversity Evaluation}

The diversity evaluation component quantifies how different the variable states observed during a test case execution are from states observed in previous executions. This diversity score provides additional feedback that complements traditional edge coverage information. We maintain a history of state vectors from recent executions, then compute similarity between the current state vector and historical states to determine how novel the current state pattern is.

The similarity computation uses type-specific metrics appropriate for each variable category. For integer variables, we use exact equality comparison for small values and bin-based comparison for larger values, which groups similar values together while distinguishing substantially different values. Character similarity computes the normalized ASCII distance between character values. String similarity combines two components: the normalized Levenshtein edit distance captures structural similarity, while the normalized length difference captures size-based differences. For pointer variables, we compute similarity based on relative address differences modulo the prime number used in encoding.

Given a current state vector $S_{curr}$ and a history of previous state vectors $\mathcal{H} = \{S_1, S_2, \ldots, S_n\}$, the diversity score is computed as one minus the maximum similarity to any historical state. Higher diversity scores indicate state patterns that differ substantially from previously observed patterns. We also incorporate a coverage ratio factor that boosts the diversity scores of inputs that reach basic blocks closer to target locations, ensuring that state diversity guidance maintains appropriate focus on the target-directed objective. The final diversity score combines the state-based diversity with the coverage ratio using configurable weights that can be adjusted based on the relative importance of state exploration versus target reaching.

Algorithm~\ref{alg:diversity} formalizes the diversity evaluation process. For each key variable, the algorithm computes type-specific similarity with the corresponding variable in historical states, then aggregates these per-variable similarities using type-based weights to produce an overall state similarity score. The diversity score is computed as one minus the maximum similarity across all historical states, yielding high scores for novel state patterns.

\begin{algorithm}[t]
\caption{State Diversity Evaluation}
\label{alg:diversity}
\begin{algorithmic}[1]
\REQUIRE Current state $S_{curr}$, State history $\mathcal{H}$, Variables $V_K$
\ENSURE Diversity score $div$
\STATE $max\_sim \leftarrow 0$
\FOR{each historical state $S_h \in \mathcal{H}$}
    \STATE $sim \leftarrow 0$, $weight\_sum \leftarrow 0$
    \FOR{each variable $v \in V_K$}
        \STATE $type \leftarrow$ GetType($v$)
        \STATE $w \leftarrow$ GetTypeWeight($type$)
        \IF{$type$ = INTEGER}
            \STATE $s \leftarrow$ IntegerSimilarity($S_{curr}[v]$, $S_h[v]$)
        \ELSIF{$type$ = CHARACTER}
            \STATE $s \leftarrow$ CharacterSimilarity($S_{curr}[v]$, $S_h[v]$)
        \ELSIF{$type$ = STRING}
            \STATE $s \leftarrow$ StringSimilarity($S_{curr}[v]$, $S_h[v]$)
        \ELSIF{$type$ = POINTER}
            \STATE $s \leftarrow$ PointerSimilarity($S_{curr}[v]$, $S_h[v]$)
        \ENDIF
        \STATE $sim \leftarrow sim + w \cdot s$
        \STATE $weight\_sum \leftarrow weight\_sum + w$
    \ENDFOR
    \STATE $sim \leftarrow sim / weight\_sum$
    \STATE $max\_sim \leftarrow$ max($max\_sim$, $sim$)
\ENDFOR
\STATE $div \leftarrow 1 - max\_sim$
\RETURN $div$
\end{algorithmic}
\end{algorithm}

\subsection{Adaptive Seed Scheduling}

The final component integrates state diversity information into the fuzzer's seed selection and energy allocation decisions. Traditional directed fuzzers prioritize seeds based primarily on edge coverage and distance to targets. GFuzz extends this with an additional state diversity dimension, computing a combined score that balances traditional coverage-based metrics with state diversity metrics. The relative weight given to each dimension is dynamically adjusted throughout the fuzzing campaign based on their respective contributions to progress.

The adaptive weight mechanism monitors two types of progress indicators during fuzzing: coverage progress measured by new edge discoveries, and state progress measured by new state patterns observed. After each fuzzing cycle, we compare the coverage progress against the state progress. If coverage-based guidance is producing more progress, we increase the weight given to traditional coverage metrics. If state-based guidance is discovering more novel behaviors, we increase the weight given to diversity metrics. This adaptation allows GFuzz to automatically emphasize whichever guidance dimension is currently more productive, improving robustness across different programs and fuzzing phases.

Seed energy allocation determines how many mutations to attempt for each selected seed. Traditional approaches base energy on distance to targets, allocating more mutations to seeds closer to targets. GFuzz extends this by also considering state diversity, boosting the energy allocated to seeds that exhibited novel state patterns. This encourages deeper exploration of the state space regions reached by diverse seeds. The energy boost factor is configurable but typically set to provide a moderate increase that rewards diversity without overwhelming the distance-based allocation.

Algorithm~\ref{alg:scheduling} presents the adaptive seed scheduling process. During each fuzzing iteration, the algorithm computes combined scores for all seeds in the corpus, selects a seed probabilistically based on these scores, determines the energy to allocate based on both distance and diversity, executes mutations, then updates the adaptive weights based on the progress achieved. This creates a feedback loop where the guidance strategy continuously adapts to the program under test and the current fuzzing state.

\begin{algorithm}[t]
\caption{Adaptive Seed Scheduling}
\label{alg:scheduling}
\begin{algorithmic}[1]
\REQUIRE Seed corpus $C$, Weights $w_{cov}$, $w_{div}$
\ENSURE Selected seed $s_{sel}$, Allocated energy $E$
\FOR{each seed $s \in C$}
    \STATE $score_{cov} \leftarrow$ ComputeCoverageScore($s$)
    \STATE $score_{div} \leftarrow$ ComputeDiversityScore($s$)
    \STATE $score_{combined}[s] \leftarrow w_{cov} \cdot score_{cov} + w_{div} \cdot score_{div}$
\ENDFOR
\STATE $s_{sel} \leftarrow$ SelectProbabilistically($C$, $score_{combined}$)
\STATE $dist \leftarrow$ GetDistanceToTarget($s_{sel}$)
\STATE $div \leftarrow$ GetDiversityScore($s_{sel}$)
\STATE $E_{base} \leftarrow$ ComputeBaseEnergy($dist$)
\STATE $E \leftarrow E_{base} \cdot (1 + \gamma \cdot div)$
\STATE ExecuteMutations($s_{sel}$, $E$)
\STATE $\Delta_{cov}$, $\Delta_{div} \leftarrow$ MeasureProgress()
\STATE $w_{cov} \leftarrow w_{cov} + \lambda \cdot (\Delta_{cov} - \Delta_{div})$
\STATE $w_{div} \leftarrow w_{div} + \lambda \cdot (\Delta_{div} - \Delta_{cov})$
\STATE NormalizeWeights($w_{cov}$, $w_{div}$)
\RETURN $s_{sel}$, $E$
\end{algorithmic}
\end{algorithm}

\section{Empirical Study}
\label{sec:empirical}

We conducted a comprehensive empirical evaluation to assess GFuzz's effectiveness compared to state-of-the-art directed fuzzing and to understand the contribution of individual components. This section presents our experimental methodology, benchmark programs, evaluation metrics, and detailed results.

\subsection{Research Questions}

Our evaluation addresses four primary research questions. RQ1 asks how GFuzz compares to AFLGo in terms of fundamental fuzzing metrics including path discovery and code coverage. RQ2 focuses on the practical effectiveness question of how well GFuzz detects known vulnerabilities compared to the baseline. RQ3 investigates the performance overhead introduced by GFuzz's state monitoring and diversity evaluation. RQ4 examines through ablation studies how each component contributes to overall effectiveness.

\subsection{Benchmark Programs and Vulnerabilities}

We selected four real-world programs containing known vulnerabilities for our evaluation. These programs represent diverse domains and vulnerability types, providing a comprehensive assessment of GFuzz's capabilities. Table~\ref{tab:benchmarks} summarizes the benchmark programs, their characteristics, and the target vulnerabilities.

\begin{table}[!htbp]
\centering
\caption{Benchmark programs and target vulnerabilities}
\label{tab:benchmarks}
\begin{tabular}{lrrl}
\toprule
\textbf{Program} & \textbf{LOC} & \textbf{Version} & \textbf{Vulnerability (CVE)} \\
\midrule
mJS & 15,423 & v1.21 & Heap buffer overflow (CVE-2018-14380) \\
binutils & 2,347,891 & v2.29 & Stack buffer overflow (CVE-2017-15939) \\
libming & 87,542 & v0.4.8 & NULL pointer dereference (CVE-2018-8964) \\
libxml2 & 287,439 & v2.9.4 & Multiple memory corruption bugs \\
\bottomrule
\end{tabular}
\end{table}

The mJS program is an embedded JavaScript engine designed for resource-constrained environments. The target vulnerability CVE-2018-14380 is a heap buffer overflow in the JSON parsing logic that occurs when specific combinations of nested objects and array sizes exceed internal buffer allocations. The binutils suite contains binary manipulation utilities used extensively in software development toolchains. CVE-2017-15939 involves a stack buffer overflow in the DWARF debug information parsing code that requires particular malformed debug entries to trigger. The libming library processes Shockwave Flash files and contains CVE-2018-8964, a NULL pointer dereference in shape record parsing that manifests under specific shape type and size combinations. The libxml2 library parses XML documents and contains multiple known memory corruption vulnerabilities that we targeted for testing.

For each program, we identified target locations corresponding to the vulnerable code paths following the methodology used in prior directed fuzzing evaluations. Target specifications included the source file name and line numbers where vulnerabilities occur. We used the same target specifications for both GFuzz and AFLGo to ensure fair comparison.

\subsection{Experimental Setup and Configuration}

All experiments were conducted on identical hardware configurations featuring Intel Xeon E5-2680 processors with 64GB RAM running Ubuntu 20.04 LTS with Linux kernel 5.4. Each program was compiled with Clang 11.0 using the LLVM-based instrumentation provided by AFLGo and extended by GFuzz. We enabled AddressSanitizer for all builds to detect memory errors with high precision.

We allocated a time budget of six hours per trial for each configuration, which represents a realistic fuzzing duration commonly used in practice. The fuzzing campaigns employed the exponential annealing power schedule recommended for directed fuzzing, with an exploitation time parameter set to 45 minutes. This schedule begins with high exploration to discover initial paths to targets, then gradually transitions toward exploitation focused on target neighborhoods.

For GFuzz-specific parameters, we set the distance threshold for key variable identification to three hops based on preliminary experiments. The state history size was configured to maintain the 100 most recent distinct state patterns, which provides sufficient memory for diversity comparison without excessive storage overhead. The learning rate for adaptive weight updates was set to 0.1, and the energy boost coefficient for diverse seeds was set to 0.5. These parameters were kept constant across all benchmark programs to demonstrate generality.

Each configuration was executed ten times with different random seeds to account for the inherent randomness in fuzzing. We collected detailed logs including execution traces, coverage information, and crash reports for statistical analysis. The seed corpus for each program was constructed from existing test cases in the program repositories, ensuring that both GFuzz and AFLGo started from identical initial conditions.

\subsection{Evaluation Metrics}

We measured multiple metrics to comprehensively assess fuzzing effectiveness. Unique paths discovered counts the number of distinct execution paths exercised during fuzzing, indicating exploration breadth. Edge coverage measures the percentage of control flow graph edges executed, reflecting overall code coverage achieved. Unique crashes counts the number of distinct crash signatures detected, where crashes are deduplicated based on stack trace hashing to avoid counting the same bug multiple times. Time to first crash records the elapsed time until the first crash is discovered, indicating how quickly the fuzzer finds vulnerabilities. Execution throughput measures the number of test case executions per second, reflecting the performance overhead of instrumentation and guidance mechanisms.

\subsection{RQ1: Path Discovery and Code Coverage}

Table~\ref{tab:coverage} presents the path discovery and edge coverage results comparing GFuzz against AFLGo across all benchmark programs. The results demonstrate consistent and substantial improvements from incorporating variable state diversity into directed fuzzing.

\begin{table*}[t]
\centering
\caption{Path discovery and edge coverage comparison between GFuzz and AFLGo (average over 10 trials)}
\label{tab:coverage}
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{3}{c}{\textbf{Unique Paths Discovered}} & \multicolumn{3}{c}{\textbf{Edge Coverage (\%)}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Program} & \textbf{AFLGo} & \textbf{GFuzz} & \textbf{Improvement} & \textbf{AFLGo} & \textbf{GFuzz} & \textbf{Improvement} \\
\midrule
mJS & 2,847 $\pm$ 124 & 3,312 $\pm$ 138 & +16.3\% & 58.4 $\pm$ 2.1 & 64.2 $\pm$ 2.3 & +9.9\% \\
binutils & 5,621 $\pm$ 287 & 6,704 $\pm$ 312 & +19.3\% & 42.7 $\pm$ 1.8 & 48.5 $\pm$ 1.9 & +13.6\% \\
libming & 4,193 $\pm$ 198 & 4,856 $\pm$ 215 & +15.8\% & 51.2 $\pm$ 2.4 & 57.8 $\pm$ 2.6 & +12.9\% \\
libxml2 & 3,765 $\pm$ 165 & 4,498 $\pm$ 182 & +19.5\% & 46.3 $\pm$ 1.9 & 51.7 $\pm$ 2.1 & +11.7\% \\
\midrule
\textbf{Average} & -- & -- & \textbf{+17.7\%} & -- & -- & \textbf{+12.0\%} \\
\bottomrule
\end{tabular}
\end{table*}

For path discovery, GFuzz discovered between 15.8\% and 19.5\% more unique paths than AFLGo across the benchmarks, with an average improvement of 17.7\%. The largest improvement occurred for binutils and libxml2, both of which are larger programs with complex parsing logic where variable state diversity particularly helps explore different execution branches. The improvements are statistically significant with p-values below 0.01 using the Mann-Whitney U test, indicating that the differences are unlikely due to random variation.

For edge coverage, GFuzz achieved between 9.9\% and 13.6\% higher coverage than AFLGo, averaging 12.0\% improvement. The binutils benchmark showed the largest coverage improvement at 13.6\%, likely because its extensive debug information parsing code contains many data-dependent branches where variable state diversity effectively guides exploration. The coverage improvements demonstrate that state diversity guidance helps the fuzzer discover code regions that purely control-flow-based guidance misses.

Figure~\ref{fig:coverage_over_time} shows how edge coverage evolves over time during fuzzing campaigns for the libxml2 benchmark. The curves represent averages across ten trials with shaded regions indicating standard deviations. GFuzz consistently achieves higher coverage than AFLGo throughout the fuzzing campaign, with the gap widening over time as state diversity guidance accumulates advantages through better exploration of the state space around targets.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\columnwidth]{figures/coverage_over_time.pdf}
\caption{Edge coverage over time for libxml2, showing GFuzz (blue) consistently outperforming AFLGo (orange) throughout the six-hour fuzzing campaign.}
\label{fig:coverage_over_time}
\end{figure}

\subsection{RQ2: Vulnerability Detection}

Table~\ref{tab:crashes} presents the vulnerability detection results, showing the number of unique crashes discovered and the time required to find the first crash for each benchmark.

\begin{table}[!htbp]
\centering
\caption{Vulnerability detection comparison between GFuzz and AFLGo}
\label{tab:crashes}
\begin{tabular}{lrrrr}
\toprule
& \multicolumn{2}{c}{\textbf{Unique Crashes}} & \multicolumn{2}{c}{\textbf{Time to First (min)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Program} & \textbf{AFLGo} & \textbf{GFuzz} & \textbf{AFLGo} & \textbf{GFuzz} \\
\midrule
mJS & 18 & 24 (+33.3\%) & 42 & 28 (-33.3\%) \\
binutils & 31 & 37 (+19.4\%) & 58 & 39 (-32.8\%) \\
libming & 14 & 18 (+28.6\%) & 67 & 51 (-23.9\%) \\
libxml2 & 22 & 27 (+22.7\%) & 48 & 35 (-27.1\%) \\
\midrule
\textbf{Average} & -- & \textbf{+26.0\%} & -- & \textbf{-29.2\%} \\
\bottomrule
\end{tabular}
\end{table}

GFuzz discovered substantially more unique crashes than AFLGo across all benchmarks, with improvements ranging from 19.4\% for binutils to 33.3\% for mJS, averaging 26.0\% more crashes. The mJS result is particularly notable because the target vulnerability requires specific combinations of nesting depth and buffer size values, precisely the type of data-dependent condition that state diversity guidance effectively explores. Statistical significance testing confirms these improvements are meaningful with p-values below 0.05.

For time to first crash, GFuzz reduced the time by an average of 29.2\% compared to AFLGo. The fastest improvement occurred for mJS where GFuzz found the first crash in 28 minutes compared to AFLGo's 42 minutes, representing a 33.3\% reduction. This faster vulnerability detection is practically valuable in scenarios such as continuous integration testing where developers want rapid feedback on whether code changes introduce vulnerabilities.

\subsection{RQ3: Performance Overhead}

Table~\ref{tab:overhead} presents execution throughput measurements showing the performance overhead introduced by GFuzz's state monitoring and diversity evaluation mechanisms.

\begin{table}[!htbp]
\centering
\caption{Execution throughput comparison (executions per second)}
\label{tab:overhead}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Program} & \textbf{Baseline} & \textbf{AFLGo} & \textbf{GFuzz} & \textbf{GFuzz vs Base} & \textbf{GFuzz vs AFLGo} \\
\midrule
mJS & 1,842 & 1,654 & 1,512 & -17.9\% & -8.6\% \\
binutils & 2,156 & 1,923 & 1,754 & -18.6\% & -8.8\% \\
libming & 1,687 & 1,518 & 1,389 & -17.7\% & -8.5\% \\
libxml2 & 1,934 & 1,741 & 1,587 & -17.9\% & -8.8\% \\
\midrule
\textbf{Average} & -- & -- & -- & \textbf{-18.0\%} & \textbf{-8.7\%} \\
\bottomrule
\end{tabular}
\end{table}

The baseline measurements represent execution without directed fuzzing instrumentation, AFLGo shows the overhead of basic block distance tracking, and GFuzz includes additional variable state monitoring. GFuzz introduces an average overhead of 18.0\% compared to the uninstrumented baseline and 8.7\% compared to AFLGo. This overhead stems primarily from the additional instructions inserted to encode and record variable states in shared memory.

While this overhead reduces execution throughput, it represents an acceptable tradeoff given the substantial improvements in vulnerability detection effectiveness. The overhead is lower than that of heavyweight program analysis techniques such as symbolic execution or taint tracking, which often introduce orders of magnitude slowdowns. The modest overhead allows GFuzz to maintain high fuzzing throughput while gaining significant benefits from state diversity guidance.

\subsection{RQ4: Ablation Study}

To understand how individual components contribute to GFuzz's overall effectiveness, we conducted an ablation study with three configurations: GFuzz-NoFilter disables the multi-strategy variable filtering and tracks all variables in distance-relevant functions, GFuzz-NoAdapt disables the adaptive weight adjustment mechanism and uses fixed equal weights for coverage and diversity scores, and GFuzz-Full represents the complete implementation with all components enabled.

Table~\ref{tab:ablation} presents ablation study results on the libxml2 benchmark, showing that each component makes meaningful contributions to overall effectiveness.

\begin{table}[!htbp]
\centering
\caption{Ablation study results on libxml2 benchmark}
\label{tab:ablation}
\begin{tabular}{lrrr}
\toprule
\textbf{Configuration} & \textbf{Unique Paths} & \textbf{Edge Coverage (\%)} & \textbf{Unique Crashes} \\
\midrule
AFLGo & 3,765 & 46.3 & 22 \\
GFuzz-NoFilter & 4,012 (+6.6\%) & 49.1 (+6.0\%) & 23 (+4.5\%) \\
GFuzz-NoAdapt & 4,287 (+13.9\%) & 50.4 (+8.9\%) & 25 (+13.6\%) \\
GFuzz-Full & 4,498 (+19.5\%) & 51.7 (+11.7\%) & 27 (+22.7\%) \\
\bottomrule
\end{tabular}
\end{table}

The GFuzz-NoFilter configuration shows only modest improvements over AFLGo, demonstrating that variable filtering is essential. Without filtering, the overhead of tracking many irrelevant variables undermines throughput while the diversity guidance becomes diluted with noise from peripheral variables. The GFuzz-NoAdapt configuration performs substantially better than NoFilter but still falls short of the full implementation, showing that adaptive weight adjustment provides significant value by dynamically emphasizing whichever guidance dimension is currently most productive. The full GFuzz implementation achieves the best results across all metrics, validating that all components work synergistically to enhance fuzzing effectiveness.

\section{Discussion}
\label{sec:discussion}

\subsection{Implications and Insights}

Our evaluation results provide several important insights about variable state diversity-guided directed fuzzing. First, the consistent improvements across all benchmarks demonstrate that incorporating data-state awareness addresses a genuine gap in existing directed fuzzing approaches. The magnitude of improvements, particularly for crash detection with an average 26\% increase, suggests that many vulnerabilities indeed require specific variable state conditions that purely control-flow-guided approaches struggle to efficiently explore.

Second, the ablation study confirms that careful component design matters significantly. The multi-strategy variable identification successfully focuses monitoring on relevant variables, as evidenced by the poor performance when this filtering is removed. The adaptive weight mechanism provides robustness across different programs and fuzzing phases by automatically adjusting guidance emphasis based on observed progress. These design choices differentiate GFuzz from a naive approach that simply tracks all variables or applies fixed guidance strategies.

Third, the performance overhead analysis reveals that the benefits substantially outweigh the costs. While GFuzz introduces approximately 18\% throughput reduction compared to uninstrumented execution, this overhead enables a 26\% improvement in crash detection and 29\% faster time to first crash. For practitioners focused on vulnerability detection rather than pure throughput maximization, this tradeoff strongly favors GFuzz.

Fourth, the time-series analysis of coverage evolution shows that state diversity guidance provides cumulative advantages throughout fuzzing campaigns. Rather than providing only initial gains that plateau, GFuzz maintains superior exploration effectiveness over extended time periods. This suggests that state diversity helps avoid local exploration traps where fuzzers repeatedly test similar variable states without making progress toward novel behaviors.

\subsection{When GFuzz Excels}

GFuzz demonstrates particular effectiveness for programs and vulnerability types with certain characteristics. Data-dependent vulnerabilities where bugs manifest only under specific variable value combinations represent the ideal use case, as evidenced by the strong results on mJS where the vulnerability requires particular depth and size relationships. Programs with complex parsing and data processing logic benefit substantially because these programs contain many data-dependent branches where variable states influence execution flow and vulnerability triggering. Directed testing scenarios where specific code locations are targeted align well with GFuzz's design, making it well-suited for patch testing, security audits focused on critical components, and reproduction of field-reported crashes.

\subsection{Limitations and Threats to Validity}

Several limitations warrant discussion. Our variable identification heuristics may miss some relevant variables or include some irrelevant ones. More sophisticated program analysis techniques such as data dependency analysis or symbolic execution could potentially improve precision, though likely at higher computational cost. The type-specific similarity metrics we employ are relatively straightforward. More advanced metrics including learned similarity functions or domain-specific comparison strategies might further enhance effectiveness. Our evaluation focused on C programs with known vulnerabilities. Generalization to other programming languages, application domains, and vulnerability types requires additional validation.

Regarding threats to validity, internal validity concerns arise from the inherent randomness in fuzzing. We addressed this through multiple trials with different random seeds and statistical significance testing to ensure reported differences are meaningful rather than artifacts of random variation. External validity limitations stem from our benchmark selection. While we chose diverse real-world programs with various vulnerability types, the results may not generalize to all possible programs and vulnerabilities. Construct validity questions whether our metrics truly capture fuzzing effectiveness. We used widely accepted metrics including path discovery, coverage, and crash detection that represent standard practice in fuzzing research.

\subsection{Future Research Directions}

Several promising avenues for future work emerge from this research. Advanced variable identification techniques incorporating inter-procedural data flow analysis, taint tracking, or program slicing could more precisely identify variables relevant to target locations. Machine learning approaches might learn optimal variable selection strategies from historical fuzzing data across multiple programs. Structure-aware state tracking could extend our approach to handle complex data structures such as trees, graphs, and nested records more effectively than our current scalar variable focus. Learned similarity metrics could adapt to program-specific patterns rather than using fixed type-based comparisons. Hybrid fuzzing integration could combine GFuzz with symbolic execution or concolic testing to handle complex path constraints that neither technique handles well independently. Finally, distributed fuzzing adaptation could scale state diversity guidance to parallel fuzzing environments where multiple instances coordinate their exploration.

\section{Related Work}
\label{sec:related}

\subsection{Greybox Fuzzing Techniques}

American Fuzzy Lop pioneered coverage-guided greybox fuzzing by using lightweight edge coverage instrumentation to guide mutation-based test generation. Numerous subsequent works have enhanced various aspects of greybox fuzzing. AFLFast introduced power schedules based on Markov chain analysis of the fuzzing process to allocate mutation energy more effectively. FairFuzz targeted rare branches that are infrequently exercised to improve coverage of hard-to-reach code regions. MOpt applied particle swarm optimization to learn effective mutation operator schedules. These works focus on improving exploration of the control-flow dimension, which GFuzz complements with data-state awareness.

\subsection{Directed Greybox Fuzzing}

AFLGo introduced directed greybox fuzzing by computing static distances to target locations and using these distances to guide fuzzing. Hawkeye improved upon AFLGo with adaptive strategies for seed prioritization and power scheduling based on runtime feedback. UAFLGo specialized directed fuzzing for use-after-free vulnerabilities through use-def analysis and pointer tracking. BEACON enhanced directedness through multiple distance metrics and hybrid guidance strategies. Our work differs from these approaches by incorporating variable state diversity as an additional guidance dimension rather than refining distance-based control-flow guidance.

\subsection{Data-Aware Fuzzing}

VUzzer used static and dynamic analysis to learn application-specific data characteristics and guide mutation toward interesting input features. Angora employed gradient descent on execution traces to solve complex path constraints through input-to-state correspondence. GREYONE applied data-flow analysis to identify value-sensitive bytes in inputs that likely influence interesting program behaviors. Redqueen observed input-to-state correspondences without heavyweight symbolic execution to efficiently handle magic byte comparisons. These techniques focus on general fuzzing scenarios and primarily address the challenge of solving complex constraints, whereas GFuzz specifically targets directed fuzzing and explores variable state diversity around target locations.

\subsection{Hybrid Fuzzing Approaches}

Driller combined fuzzing with selective symbolic execution, using fuzzing for shallow exploration and symbolic execution when fuzzing gets stuck. QSYM implemented an efficient concolic execution engine optimized for hybrid fuzzing scenarios. Matryoshka addressed deeply nested branches through focused symbolic execution. DigFuzz specialized hybrid fuzzing for probability-based behaviors by identifying probabilistic branches and transforming programs to avoid them. These hybrid approaches combine multiple techniques to achieve benefits of both fuzzing and symbolic execution. GFuzz could potentially be combined with hybrid approaches to provide state diversity guidance in addition to constraint solving capabilities.

\subsection{Program Analysis for Fuzzing}

Various program analysis techniques have been applied to improve fuzzing effectiveness. TaintScope used taint analysis to identify checksums and other input transformations that block fuzzing progress. SlowFuzz targeted performance bugs through resource usage analysis. SemFuzz applied semantic reasoning about input structure to generate more valid inputs for complex file formats. NAUTILUS used grammar-based input generation guided by code coverage feedback. These works demonstrate the value of program analysis for enhancing fuzzing, similar to how GFuzz uses analysis to identify key variables, though focusing on different aspects of the fuzzing problem.

\section{Conclusion}
\label{sec:conclusion}

This paper presented GFuzz, a novel directed fuzzing approach that enhances vulnerability detection by systematically incorporating variable state diversity into the fuzzing process. Our key insight is that many vulnerabilities require not only reaching specific code locations but also satisfying particular data conditions involving program variables. By identifying key variables relevant to target locations, monitoring their runtime states efficiently, evaluating state diversity, and adaptively balancing coverage-based and state-based guidance, GFuzz significantly improves upon existing directed fuzzing techniques.

Our comprehensive evaluation on four real-world programs containing known vulnerabilities demonstrated substantial improvements over AFLGo across multiple metrics. GFuzz discovered 17.7\% more execution paths, achieved 12.0\% better code coverage, detected 26.0\% more crashes, and reduced time to first crash by 29.2\% on average, while introducing acceptable 18.0\% performance overhead. The ablation study confirmed that each component of our approach contributes meaningfully to overall effectiveness, with the full implementation outperforming partial configurations.

The success of GFuzz demonstrates that combining control-flow and data-state awareness provides a more complete approach to directed fuzzing than either dimension alone. As software systems continue growing in complexity and security importance, techniques that leverage multiple sources of feedback will become increasingly valuable for automated vulnerability detection. Our work opens numerous avenues for future research including advanced variable identification strategies, learned similarity metrics, structure-aware state tracking, and hybrid combinations with symbolic execution.

We have released the complete implementation of GFuzz as open source, along with our benchmarks and experimental data, to facilitate reproduction and future research building on this work. We believe that variable state diversity-guided fuzzing represents a promising direction for enhancing automated vulnerability detection and hope that our work inspires further investigation of data-state awareness in software testing.

\section*{CRediT authorship contribution statement}

\textbf{Anonymous Author 1}: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Writing - Original Draft, Visualization. \textbf{Anonymous Author 2}: Methodology, Software, Validation, Writing - Review \& Editing. \textbf{Anonymous Author 3}: Resources, Writing - Review \& Editing, Supervision, Project administration, Funding acquisition.

\section*{Declaration of competing interest}

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{Data availability}

The source code, benchmarks, experimental data, and analysis scripts are publicly available at \url{https://github.com/cyhhtl999520/aflgo}. The repository includes complete instructions for reproducing our experiments and replicating the results reported in this paper.

\section*{Acknowledgment}

This work was supported by [REDACTED FOR BLIND REVIEW]. We thank the anonymous reviewers for their constructive feedback that improved this paper.

\begin{thebibliography}{10}

\bibitem{afl}
M.~Zalewski, ``American Fuzzy Lop,'' \url{http://lcamtuf.coredump.cx/afl/}, 2014.

\bibitem{libfuzzer}
LLVM~Project, ``LibFuzzer - A library for coverage-guided fuzz testing,'' \url{https://llvm.org/docs/LibFuzzer.html}, 2015.

\bibitem{aflgo}
M.~Bhme, V.-T.~Pham, M.-D.~Nguyen, and A.~Roychoudhury, ``Directed greybox fuzzing,'' in \emph{Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2017, pp. 2329--2344.

\bibitem{hawkeye}
H.~Chen, Y.~Xue, Y.~Li, B.~Chen, X.~Xie, X.~Wu, and Y.~Liu, ``Hawkeye: Towards a desired directed grey-box fuzzer,'' in \emph{Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2018, pp. 2095--2108.

\bibitem{uaflgo}
W.~Wang, J.~Xu, Y.~Zhang, and X.~Zhang, ``UAFLGo: Directed fuzzing for use-after-free vulnerabilities,'' in \emph{Proceedings of the 2020 IEEE Symposium on Security and Privacy (S\&P)}, 2020, pp. 1573--1588.

\bibitem{aflfast}
M.~Bhme, V.-T.~Pham, and A.~Roychoudhury, ``Coverage-based greybox fuzzing as Markov chain,'' in \emph{Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2016, pp. 1032--1043.

\bibitem{fairfuzz}
C.~Lemieux and K.~Sen, ``FairFuzz: A targeted mutation strategy for increasing greybox fuzz testing coverage,'' in \emph{Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE)}, 2018, pp. 475--485.

\bibitem{mopt}
C.~Lyu, S.~Ji, C.~Zhang, Y.~Li, W.-H.~Lee, Y.~Song, and R.~Beyah, ``MOPT: Optimized mutation scheduling for fuzzers,'' in \emph{Proceedings of the 28th USENIX Security Symposium}, 2019, pp. 1949--1966.

\bibitem{redqueen}
C.~Aschermann, S.~Schumilo, T.~Blazytko, R.~Gawlik, and T.~Holz, ``REDQUEEN: Fuzzing with input-to-state correspondence,'' in \emph{Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS)}, 2019.

\bibitem{vuzzer}
S.~Rawat, V.~Jain, A.~Kumar, L.~Cojocar, C.~Giuffrida, and H.~Bos, ``VUzzer: Application-aware evolutionary fuzzing,'' in \emph{Proceedings of the 2017 Network and Distributed System Security Symposium (NDSS)}, 2017.

\bibitem{angora}
P.~Chen and H.~Chen, ``Angora: Efficient fuzzing by principled search,'' in \emph{Proceedings of the 2018 IEEE Symposium on Security and Privacy (S\&P)}, 2018, pp. 711--725.

\bibitem{greyone}
J.~Gan, S.~Zhang, X.~Wang, Y.~Qin, J.~Yao, and Q.~Hu, ``GREYONE: Data flow sensitive fuzzing,'' in \emph{Proceedings of the 29th USENIX Security Symposium}, 2020, pp. 2577--2594.

\bibitem{qsym}
I.~Yun, S.~Lee, M.~Xu, Y.~Jang, and T.~Kim, ``QSYM: A practical concolic execution engine tailored for hybrid fuzzing,'' in \emph{Proceedings of the 27th USENIX Security Symposium}, 2018, pp. 745--761.

\bibitem{matryoshka}
P.~Jia, P.~Cui, L.~Liu, Y.~Xie, and J.~Zhang, ``Matryoshka: Fuzzing deeply nested branches,'' in \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2019, pp. 499--513.

\bibitem{klee}
C.~Cadar, D.~Dunbar, and D.~Engler, ``KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs,'' in \emph{Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI)}, 2008, pp. 209--224.

\bibitem{driller}
N.~Stephens, J.~Grosen, C.~Salls, A.~Dutcher, R.~Wang, J.~Corbetta, Y.~Shoshitaishvili, C.~Kruegel, and G.~Vigna, ``Driller: Augmenting fuzzing through selective symbolic execution,'' in \emph{Proceedings of the 2016 Network and Distributed System Security Symposium (NDSS)}, 2016.

\end{thebibliography}

\end{document}
