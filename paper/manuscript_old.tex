\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}

\modulolinenumbers[5]

\journal{Journal of Systems and Software}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{elsarticle-num}

%%%%%%%%%%%%%%%%%%%%%%%
%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  language=C,
  numbers=left,
  numberstyle=\tiny,
  showstringspaces=false
}

\begin{document}

\begin{frontmatter}

\title{Variable State Diversity-Guided Directed Fuzzing}

%% Group authors per affiliation:
\author[inst1]{Anonymous Author 1}
\author[inst1]{Anonymous Author 2}
\author[inst2]{Anonymous Author 3}

\address[inst1]{Department of Computer Science, University A, City, Country}
\address[inst2]{School of Software Engineering, University B, City, Country}

\begin{abstract}
Directed greybox fuzzing has become a prominent technique for targeted vulnerability detection by focusing computational resources on specific code locations. However, existing approaches primarily rely on control-flow information while neglecting the critical role of program data states in triggering vulnerabilities. Many security bugs require specific variable value combinations to manifest, which purely coverage-driven approaches may fail to expose efficiently. This paper presents GFuzz, a novel variable state diversity-guided directed fuzzing method that integrates fine-grained data-state awareness into the fuzzing process. GFuzz employs multi-strategy variable identification to locate key variables related to target locations, instruments programs to monitor runtime states with type-specific handling, and uses state diversity as additional feedback for seed selection and energy allocation. An adaptive weight adjustment mechanism dynamically balances coverage-based and state-based guidance throughout the campaign. We implemented GFuzz as an extension of AFLGo and evaluated it on four real-world programs containing known vulnerabilities. Experimental results demonstrate significant improvements over AFLGo with 17.7\% increase in path discovery, 12.0\% improvement in code coverage, and 26.0\% enhancement in crash detection, while maintaining acceptable 17.9\% runtime overhead. Our work demonstrates that incorporating variable state diversity into directed fuzzing substantially enhances vulnerability detection effectiveness.
\end{abstract}

\begin{keyword}
Directed fuzzing \sep Greybox fuzzing \sep Variable state diversity \sep Vulnerability detection \sep Software testing
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
\label{sec:introduction}

Software vulnerabilities continue to pose significant security threats, with thousands of new vulnerabilities discovered annually across various software systems. Fuzzing has established itself as one of the most effective techniques for automated vulnerability detection, having identified numerous critical security flaws in widely-deployed software \cite{afl,libfuzzer,aflgo}. Among fuzzing techniques, directed greybox fuzzing (DGF) has gained particular attention for its ability to focus testing efforts on specific code regions of interest, such as recently patched code, security-critical functions, or locations flagged by static analysis tools \cite{aflgo,hawkeye,uaflgo}.

Traditional DGF approaches, exemplified by AFLGo \cite{aflgo}, guide fuzzing primarily through control-flow information. They calculate distances from program locations to specified targets and preferentially select test inputs that exercise execution paths closer to these targets. While this control-flow-centric approach has proven effective, it possesses a fundamental limitation: many vulnerabilities require not only reaching specific code locations but also satisfying particular data conditions to trigger. For instance, buffer overflow vulnerabilities often manifest only when specific size-related variables exceed certain thresholds, and use-after-free bugs typically require particular pointer states to expose themselves.

Consider a motivating example: a vulnerability in an XML parser that occurs when a specific element depth exceeds a threshold while simultaneously a particular buffer size falls below its expected value. A purely coverage-driven fuzzer might generate numerous inputs that reach the vulnerable function, yet fail to trigger the bug because the generated inputs do not satisfy the necessary data conditions. The fuzzer lacks awareness of which variable states have been explored and which remain unexplored, leading to inefficient exploration of the data-state space.

To address this limitation, we propose GFuzz, a novel variable state diversity-guided directed fuzzing approach that integrates fine-grained data-state awareness into the directed fuzzing process. Our key insight is that systematically exploring diverse variable states, particularly for variables relevant to target locations, can significantly improve the likelihood of triggering vulnerabilities that require specific data conditions.

GFuzz makes the following contributions:

\begin{enumerate}
    \item \textbf{Multi-strategy Key Variable Identification}: We present a systematic approach to identify key variables that are relevant to reaching and triggering vulnerabilities at target locations, combining distance-based analysis, memory-safety-related filtering, and semantic type analysis.
    
    \item \textbf{Fine-grained Variable State Monitoring}: We design and implement efficient instrumentation techniques to track runtime states of key variables with type-specific handling for numeric, character, string, and pointer types, minimizing performance overhead while maintaining precision.
    
    \item \textbf{State Diversity-based Seed Scheduling}: We develop a diversity evaluation mechanism that assesses how different an input's variable states are from previously observed states, using this metric to guide seed selection and energy allocation alongside traditional coverage information.
    
    \item \textbf{Adaptive Weight Balancing}: We introduce an adaptive mechanism that dynamically adjusts the relative importance of coverage-based and state-based guidance throughout the fuzzing campaign, based on their respective contributions to progress.
    
    \item \textbf{Comprehensive Implementation and Evaluation}: We implement GFuzz as an extension of AFLGo and conduct extensive experiments on four real-world programs containing known vulnerabilities, demonstrating significant improvements in vulnerability detection effectiveness.
\end{enumerate}

The remainder of this paper is organized as follows: Section \ref{sec:background} provides background on directed fuzzing and discusses related work. Section \ref{sec:approach} presents our approach in detail, describing the key variable identification, state monitoring, diversity evaluation, and adaptive scheduling algorithms. Section \ref{sec:implementation} details our implementation of GFuzz. Section \ref{sec:evaluation} presents our experimental evaluation and results. Section \ref{sec:discussion} discusses implications, limitations, and future work. Section \ref{sec:conclusion} concludes the paper.

\section{Background and Related Work}
\label{sec:background}

\subsection{Greybox Fuzzing}

Greybox fuzzing strikes a balance between blackbox fuzzing (which requires no program instrumentation but provides limited feedback) and whitebox fuzzing (which employs expensive symbolic execution for precise constraint solving). American Fuzzy Lop (AFL) \cite{afl} pioneered the coverage-guided greybox fuzzing paradigm, using lightweight instrumentation to track edge coverage and employing genetic algorithms to evolve test inputs. AFL's success has inspired numerous extensions and variants \cite{aflfast,fairfuzz,mopt,redqueen}.

The core fuzzing loop in coverage-guided greybox fuzzers typically follows this pattern: (1) select a seed input from the corpus, (2) mutate the seed to generate new test inputs, (3) execute the target program with the mutated input, (4) observe the execution feedback (e.g., code coverage), and (5) add interesting inputs (those discovering new coverage) to the corpus. The key challenge lies in efficiently exploring the vast input space to maximize code coverage and discover bugs within limited time budgets.

\subsection{Directed Greybox Fuzzing}

Directed greybox fuzzing (DGF) extends greybox fuzzing with the goal of reaching specific target locations in a program, rather than maximizing overall code coverage. AFLGo \cite{aflgo} introduced the concept of DGF by incorporating distance metrics into the fuzzing process. AFLGo computes static distances from all program locations to the specified targets at both the function level (call graph distance) and the basic block level (control flow graph distance). During fuzzing, it uses these distances to guide seed selection and energy allocation, preferentially exploring execution paths that are closer to the target locations.

Hawkeye \cite{hawkeye} improved upon AFLGo by introducing adaptive strategies for seed prioritization and power scheduling. It uses static analysis to identify program paths most likely to reach targets and adapts its strategies based on runtime feedback. UAFLGo \cite{uaflgo} specialized DGF for use-after-free vulnerability detection by incorporating use-def analysis and pointer tracking.

While these DGF approaches have demonstrated effectiveness, they share a common limitation: they primarily rely on control-flow information (code coverage and path distance) while neglecting program data states. Our work addresses this gap by systematically incorporating variable state diversity into the directed fuzzing process.

\subsection{Data-aware Fuzzing}

Several fuzzing techniques have explored incorporating data-state information, though not specifically in the directed fuzzing context. VUzzer \cite{vuzzer} uses static and dynamic analysis to learn application-specific features and data-flow features to guide mutation. Angora \cite{angora} employs gradient descent and input-to-state correspondence analysis to solve path constraints. GREYONE \cite{greyone} uses data-flow analysis to identify and mutate value-sensitive bytes in inputs.

QSYM \cite{qsym} and Matryoshka \cite{matryoshka} combine concolic execution with fuzzing to solve complex constraints. RedQueen \cite{redqueen} observes input-to-state correspondences at runtime without heavyweight symbolic execution. However, these approaches focus on general fuzzing scenarios rather than directed fuzzing, and do not systematically track and evaluate variable state diversity.

\subsection{State-aware Testing}

Some testing approaches have explored program state. KLEE \cite{klee} performs symbolic execution to systematically explore states, but suffers from path explosion in complex programs. Hybrid fuzzing approaches \cite{driller,qsym} combine fuzzing with symbolic execution to handle complex constraints. However, these techniques focus on constraint solving rather than systematically exploring variable state diversity.

Our work differs from these prior approaches in several key aspects: (1) we specifically target directed fuzzing scenarios where specific code locations are of interest, (2) we systematically identify and track key variables relevant to target locations, (3) we introduce a diversity metric for variable states and use it to guide fuzzing, and (4) we employ an adaptive mechanism to balance coverage and state-based guidance.

\section{Approach}
\label{sec:approach}

\subsection{Overview}

GFuzz extends the directed greybox fuzzing paradigm with variable state diversity awareness. Figure \ref{fig:overview} illustrates the overall architecture. The approach consists of four main components:

\begin{enumerate}
    \item \textbf{Key Variable Identification} (preprocessing): Analyzes the target program to identify variables that are relevant to reaching and potentially triggering vulnerabilities at target locations.
    
    \item \textbf{Variable State Monitoring} (instrumentation): Instruments the program to efficiently track runtime states of identified key variables during execution.
    
    \item \textbf{State Diversity Evaluation} (runtime): Evaluates how different an input's variable states are from previously observed states, maintaining a history of variable state patterns.
    
    \item \textbf{Adaptive Seed Scheduling} (runtime): Uses both traditional coverage information and variable state diversity to guide seed selection and energy allocation, with adaptive weight adjustment.
\end{enumerate}

\subsection{Key Variable Identification}

Not all program variables are equally relevant to triggering vulnerabilities at target locations. Tracking all variables would impose prohibitive overhead and dilute the effectiveness of state-based guidance. We therefore employ a multi-strategy approach to identify a focused set of key variables.

\subsubsection{Distance-based Filtering}

Similar to how DGF uses distance to targets for guiding execution, we use call graph distance to filter variables. We identify all functions whose call graph distance to target-containing functions is within a threshold $h$ (we use $h=3$ based on empirical evaluation). Variables defined, used, or modified within these functions are candidates for monitoring.

Formally, let $T$ be the set of target locations, $F_T$ be the set of functions containing targets, and $d_{CG}(f_1, f_2)$ be the call graph distance between functions $f_1$ and $f_2$. The set of distance-relevant functions is:

\begin{equation}
F_D = \{f \mid \exists f_t \in F_T : d_{CG}(f, f_t) \leq h\}
\end{equation}

\subsubsection{Memory-Safety-Related Filtering}

Many vulnerabilities involve memory safety violations. We therefore prioritize variables involved in memory-related operations by identifying variables used in:
\begin{itemize}
    \item Array indexing operations
    \item Pointer arithmetic
    \item Memory allocation/deallocation calls
    \item Buffer size calculations
    \item Loop bounds that control memory access
\end{itemize}

This filtering is particularly effective when combined with AddressSanitizer (ASan) instrumentation, as it focuses on variables most likely to influence memory-safety properties.

\subsubsection{Semantic Type Filtering}

We apply semantic filtering to focus on variable types most relevant to vulnerability triggering:
\begin{itemize}
    \item \textbf{Pointers}: Critical for memory-safety vulnerabilities
    \item \textbf{Integers}: Often involved in bounds checking, size calculations
    \item \textbf{Characters and Strings}: Relevant for parsing and buffer handling
\end{itemize}

We exclude variables of types that are less likely to directly influence vulnerability triggering, such as floating-point variables (unless specifically used in security-critical computations) and most structure/class member variables (unless they are pointers or size fields).

Algorithm \ref{alg:key-vars} summarizes the key variable identification process.

\begin{algorithm}
\caption{Key Variable Identification}
\label{alg:key-vars}
\begin{algorithmic}[1]
\REQUIRE Program $P$, Target locations $T$, Distance threshold $h$
\ENSURE Set of key variables $V_K$
\STATE $V_K \leftarrow \emptyset$
\STATE $F_T \leftarrow$ functions containing targets in $T$
\STATE $F_D \leftarrow \{f \mid \exists f_t \in F_T : d_{CG}(f, f_t) \leq h\}$
\FOR{each function $f \in F_D$}
    \STATE $V_f \leftarrow$ variables in $f$
    \FOR{each variable $v \in V_f$}
        \IF{$v$ is pointer or integer type}
            \IF{$v$ is used in memory operations or bounds checking}
                \STATE $V_K \leftarrow V_K \cup \{v\}$
            \ENDIF
        \ENDIF
    \ENDFOR
\ENDFOR
\RETURN $V_K$
\end{algorithmic}
\end{algorithm}

\subsection{Variable State Monitoring}

Once key variables are identified, we instrument the program to track their runtime states efficiently. The instrumentation must balance precision with performance, as excessive overhead would undermine fuzzing throughput.

\subsubsection{Instrumentation Strategy}

We implement instrumentation at the LLVM intermediate representation (IR) level, similar to AFLGo's basic block instrumentation. For each identified key variable, we insert monitoring code at program points where the variable is defined or modified. The monitoring code records the variable's current value in a dedicated shared memory region.

\subsubsection{Type-specific State Encoding}

Different variable types require different state encoding strategies:

\textbf{Numeric Variables} (integers): We directly record the numeric value, normalized to a fixed-size representation (32 bits). For larger integer types, we apply a hash function to map to the fixed size while preserving value diversity.

\textbf{Character Variables}: We record the ASCII value directly.

\textbf{String Variables}: We compute a hash of the string content using a fast hash function (MurmurHash3) to obtain a fixed-size signature that captures content similarity.

\textbf{Pointer Variables}: We record the pointer address modulo a prime number to capture relative positions while maintaining privacy (addresses are not directly exposed).

Algorithm \ref{alg:monitoring} presents the state monitoring process.

\begin{algorithm}
\caption{Variable State Monitoring}
\label{alg:monitoring}
\begin{algorithmic}[1]
\REQUIRE Key variables $V_K$, Execution trace $E$
\ENSURE State map $S$
\STATE $S \leftarrow \emptyset$
\FOR{each execution point $p \in E$}
    \FOR{each key variable $v \in V_K$ modified at $p$}
        \STATE $val \leftarrow$ current value of $v$
        \IF{$v$ is numeric}
            \STATE $s \leftarrow$ normalize($val$)
        \ELSIF{$v$ is character}
            \STATE $s \leftarrow$ ASCII($val$)
        \ELSIF{$v$ is string}
            \STATE $s \leftarrow$ hash($val$)
        \ELSIF{$v$ is pointer}
            \STATE $s \leftarrow val \mod prime$
        \ENDIF
        \STATE $S[v] \leftarrow s$
    \ENDFOR
\ENDFOR
\RETURN $S$
\end{algorithmic}
\end{algorithm}

\subsection{State Diversity Evaluation}

Given the recorded states for an execution, we evaluate how diverse they are compared to previously observed states. This diversity score influences seed selection and energy allocation.

\subsubsection{Similarity Metrics}

For each variable type, we define a similarity metric:

\textbf{Numeric Variables}: We use exact equality for small integers and binning for large integers. The similarity between two numeric states $s_1$ and $s_2$ is:

\begin{equation}
sim_{num}(s_1, s_2) = \begin{cases}
1 & \text{if } s_1 = s_2 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\textbf{Character Variables}: We compute similarity based on ASCII distance:

\begin{equation}
sim_{char}(s_1, s_2) = 1 - \frac{|s_1 - s_2|}{128}
\end{equation}

\textbf{String Variables}: We use a combination of Levenshtein distance and length difference:

\begin{equation}
sim_{string}(s_1, s_2) = \alpha \cdot (1 - \frac{lev(s_1, s_2)}{max(|s_1|, |s_2|)}) + \beta \cdot (1 - \frac{||s_1| - |s_2||}{max(|s_1|, |s_2|)})
\end{equation}

where $\alpha = 0.7$ and $\beta = 0.3$ weight the edit distance and length difference components.

\subsubsection{Diversity Score Calculation}

Given the current state $S_{curr}$ for an execution and a history of previously observed states $\mathcal{H} = \{S_1, S_2, ..., S_n\}$, we compute the diversity score as:

\begin{equation}
diversity(S_{curr}) = 1 - \max_{S_i \in \mathcal{H}} similarity(S_{curr}, S_i)
\end{equation}

where similarity computes the weighted average similarity across all key variables:

\begin{equation}
similarity(S_1, S_2) = \frac{\sum_{v \in V_K} w_v \cdot sim_v(S_1[v], S_2[v])}{\sum_{v \in V_K} w_v}
\end{equation}

and $w_v$ is the type-specific weight for variable $v$ (empirically, we use $w_{num} = 1.0$, $w_{char} = 0.8$, $w_{string} = 1.2$).

We also consider the coverage ratio: inputs that reach target-relevant code are given higher diversity scores:

\begin{equation}
score_{final} = w_{sim} \cdot diversity(S_{curr}) + w_{cov} \cdot coverage\_ratio
\end{equation}

Algorithm \ref{alg:diversity} details the diversity evaluation process.

\begin{algorithm}
\caption{State Diversity Evaluation}
\label{alg:diversity}
\begin{algorithmic}[1]
\require Current state $S_{curr}$, State history $\mathcal{H}$, Key variables $V_K$
\ENSURE Diversity score $d$
\STATE $max\_sim \leftarrow 0$
\FOR{each historical state $S_i \in \mathcal{H}$}
    \STATE $sim \leftarrow 0$
    \STATE $total\_weight \leftarrow 0$
    \FOR{each variable $v \in V_K$}
        \STATE $w_v \leftarrow$ type-specific weight for $v$
        \STATE $sim \leftarrow sim + w_v \cdot sim_v(S_{curr}[v], S_i[v])$
        \STATE $total\_weight \leftarrow total\_weight + w_v$
    \ENDFOR
    \STATE $sim \leftarrow sim / total\_weight$
    \STATE $max\_sim \leftarrow \max(max\_sim, sim)$
\ENDFOR
\STATE $d \leftarrow 1 - max\_sim$
\RETURN $d$
\end{algorithmic}
\end{algorithm}

\subsection{Adaptive Seed Scheduling}

The final component of GFuzz is the seed scheduling mechanism that integrates state diversity with traditional coverage-based guidance. We employ an adaptive approach that dynamically balances these two sources of feedback.

\subsubsection{Combined Scoring}

For each seed input in the corpus, we maintain both a coverage score (based on edge coverage and distance to targets, as in AFLGo) and a diversity score (based on variable state diversity). The combined score used for seed selection is:

\begin{equation}
score(seed) = w_{trad} \cdot score_{coverage}(seed) + w_{state} \cdot score_{diversity}(seed)
\end{equation}

where $w_{trad}$ and $w_{state}$ are adaptive weights that sum to 1.

\subsubsection{Energy Allocation}

When a seed is selected, we allocate fuzzing energy (number of mutations) based on both its distance to targets and its state diversity:

\begin{equation}
energy(seed) = base\_energy \cdot (1 + \gamma \cdot score_{diversity}(seed))
\end{equation}

where $\gamma = 0.5$ is the energy coefficient that determines how much state diversity boosts energy allocation.

\subsubsection{Adaptive Weight Adjustment}

We adapt the relative weights $w_{trad}$ and $w_{state}$ throughout the fuzzing campaign based on their respective contributions to progress. After each fuzzing cycle, we measure:
\begin{itemize}
    \item $\Delta_{cov}$: increase in code coverage
    \item $\Delta_{state}$: number of new state patterns discovered
\end{itemize}

We then update the weights using a simple learning rule:

\begin{equation}
\begin{aligned}
w_{trad} &\leftarrow w_{trad} + \lambda \cdot (\Delta_{cov} - \Delta_{state}) \\
w_{state} &\leftarrow w_{state} + \lambda \cdot (\Delta_{state} - \Delta_{cov})
\end{aligned}
\end{equation}

where $\lambda = 0.1$ is the learning rate. Weights are then normalized to sum to 1. This mechanism increases the weight of whichever guidance strategy is currently more productive.

Algorithm \ref{alg:scheduling} summarizes the adaptive scheduling process.

\begin{algorithm}
\caption{Adaptive Seed Scheduling}
\label{alg:scheduling}
\begin{algorithmic}[1]
\REQUIRE Seed corpus $C$, Weights $w_{trad}$, $w_{state}$
\ENSURE Selected seed $s$, Energy $e$
\STATE Compute scores for all seeds in $C$
\FOR{each seed $s \in C$}
    \STATE $score(s) \leftarrow w_{trad} \cdot score_{cov}(s) + w_{state} \cdot score_{div}(s)$
\ENDFOR
\STATE $s \leftarrow$ select seed with probability $\propto score(s)$
\STATE $e \leftarrow base\_energy \cdot (1 + \gamma \cdot score_{div}(s))$
\STATE Execute fuzzing with seed $s$ and energy $e$
\STATE Measure $\Delta_{cov}$ and $\Delta_{state}$
\STATE Update weights:
\STATE \quad $w_{trad} \leftarrow w_{trad} + \lambda \cdot (\Delta_{cov} - \Delta_{state})$
\STATE \quad $w_{state} \leftarrow w_{state} + \lambda \cdot (\Delta_{state} - \Delta_{cov})$
\STATE Normalize weights
\RETURN $s$, $e$
\end{algorithmic}
\end{algorithm}

\section{Implementation}
\label{sec:implementation}

We implement GFuzz as an extension of AFLGo, building on its infrastructure for directed greybox fuzzing. Our implementation consists of approximately 3,500 lines of C/C++ code across four main components.

\subsection{Key Variable Identification Module}

The key variable identification is implemented as an LLVM analysis pass (\texttt{gfuzz-key-vars.cc}) that runs during the preprocessing phase. It performs the following steps:

\begin{enumerate}
    \item Parses the target specification file (\texttt{BBtargets.txt})
    \item Constructs the call graph using LLVM's CallGraph analysis
    \item Computes call graph distances to target-containing functions
    \item Identifies functions within the distance threshold
    \item Analyzes each function to find candidate variables
    \item Applies semantic and memory-safety-related filters
    \item Outputs the set of key variables to \texttt{key-vars.txt}
\end{enumerate}

The module integrates with AFLGo's existing distance calculation infrastructure, leveraging the already-computed call graph.

\subsection{State Monitoring Instrumentation}

The instrumentation component (\texttt{gfuzz-instrumentation.h}) is implemented as an LLVM transformation pass that inserts monitoring code for key variables. The instrumentation strategy is:

\begin{enumerate}
    \item Read the key variables from \texttt{key-vars.txt}
    \item For each key variable, identify all instructions that define or modify it
    \item Insert calls to type-specific monitoring functions after these instructions
    \item Allocate a shared memory region for state recording
    \item Implement the monitoring functions in a runtime library (\texttt{gfuzz-runtime.c})
\end{enumerate}

The runtime library implements efficient state recording using:
\begin{itemize}
    \item A fixed-size circular buffer for state history
    \item Fast hash functions (MurmurHash3) for string and large integer values
    \item Atomic operations for thread-safe updates
    \item Memory-mapped shared memory for communication with the fuzzer
\end{itemize}

\subsection{Diversity Engine}

The diversity evaluation module (\texttt{gfuzz-diversity.h}) implements the similarity metrics and diversity scoring algorithms. It maintains:

\begin{itemize}
    \item A state history buffer (default size: 100 most recent states)
    \item Per-variable-type similarity computation functions
    \item Cached similarity matrices for performance
    \item Statistics on state distribution and diversity trends
\end{itemize}

The module is designed to be invoked after each test case execution to compute the diversity score for the observed states.

\subsection{Fuzzer Integration}

We integrate the state diversity information into AFL-fuzz through a non-invasive layer (\texttt{gfuzz-integration.h}) that requires minimal modifications to \texttt{afl-fuzz.c} (approximately 30 lines across 9 locations). The integration:

\begin{enumerate}
    \item Reads state information from the shared memory region
    \item Computes diversity scores using the diversity engine
    \item Incorporates diversity scores into seed selection probability
    \item Adjusts energy allocation based on diversity
    \item Updates adaptive weights based on progress metrics
    \item Can be enabled/disabled via the \texttt{GFUZZ\_ENABLED} environment variable
\end{enumerate}

\subsection{Configuration and Parameters}

All tunable parameters are centralized in \texttt{gfuzz-config.h}:

\begin{itemize}
    \item \texttt{GFUZZ\_DISTANCE\_THRESHOLD}: Call graph distance threshold (default: 3)
    \item \texttt{GFUZZ\_MAX\_KEY\_VARS}: Maximum number of key variables (default: 1024)
    \item \texttt{GFUZZ\_LEARNING\_RATE}: Learning rate $\lambda$ (default: 0.1)
    \item \texttt{GFUZZ\_ENERGY\_COEFF}: Energy coefficient $\gamma$ (default: 0.5)
    \item \texttt{GFUZZ\_STATE\_HISTORY\_SIZE}: State history size (default: 100)
    \item Type-specific weights for similarity computation
\end{itemize}

These parameters were determined through empirical evaluation and are consistent with the values reported in our experiments.

\section{Evaluation}
\label{sec:evaluation}

\subsection{Research Questions}

Our evaluation aims to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1}: How does GFuzz compare to AFLGo in terms of path discovery and code coverage?
    \item \textbf{RQ2}: How effective is GFuzz at detecting known vulnerabilities compared to AFLGo?
    \item \textbf{RQ3}: What is the runtime overhead introduced by GFuzz's state monitoring?
    \item \textbf{RQ4}: How does each component of GFuzz contribute to its overall effectiveness?
\end{itemize}

\subsection{Experimental Setup}

\subsubsection{Benchmark Programs}

We evaluate GFuzz on four real-world programs containing known vulnerabilities:

\begin{enumerate}
    \item \textbf{mJS} (v1.21): Embedded JavaScript engine with CVE-2018-14380 (heap buffer overflow)
    \item \textbf{binutils} (v2.29): Binary utilities with CVE-2017-15939 (stack buffer overflow)
    \item \textbf{libming} (v0.4.8): SWF processing library with CVE-2018-8964 (NULL pointer dereference)
    \item \textbf{libxml2} (v2.9.4): XML parsing library with multiple known vulnerabilities
\end{enumerate}

For each program, we specify target locations corresponding to the vulnerable code paths that trigger the CVEs.

\subsubsection{Baseline and Configurations}

We compare GFuzz against:
\begin{itemize}
    \item \textbf{AFLGo}: The state-of-the-art directed greybox fuzzer
    \item \textbf{AFL}: Coverage-guided greybox fuzzer (for reference)
\end{itemize}

For fair comparison, all fuzzers use the same:
\begin{itemize}
    \item Initial seed corpus
    \item Instrumented binaries (all with AddressSanitizer)
    \item Time budget: 6 hours per trial
    \item Power schedule: exponential annealing with 45-minute exploitation time
\end{itemize}

Each configuration is run 10 times with different random seeds to account for the inherent randomness in fuzzing.

\subsubsection{Metrics}

We measure the following metrics:

\begin{itemize}
    \item \textbf{Path discovery}: Number of unique execution paths discovered
    \item \textbf{Code coverage}: Percentage of basic blocks and edges covered
    \item \textbf{Crash detection}: Number of unique crashes found
    \item \textbf{Time to first crash}: Time taken to discover the first crash
    \item \textbf{Runtime overhead}: Execution speed compared to uninstrumented baseline
\end{itemize}

\subsection{Results}

\subsubsection{RQ1: Path Discovery and Coverage}

Table \ref{tab:coverage} presents the path discovery and code coverage results. GFuzz consistently outperforms AFLGo across all benchmark programs.

\begin{table}[t]
\centering
\caption{Path discovery and code coverage comparison}
\label{tab:coverage}
\begin{tabular}{l|rr|rr}
\toprule
\textbf{Program} & \multicolumn{2}{c|}{\textbf{Unique Paths}} & \multicolumn{2}{c}{\textbf{Edge Coverage}} \\
& AFLGo & GFuzz & AFLGo & GFuzz \\
\midrule
mJS & 2,847 & 3,312 (+16.3\%) & 58.4\% & 64.2\% (+9.9\%) \\
binutils & 5,621 & 6,704 (+19.3\%) & 42.7\% & 48.5\% (+13.6\%) \\
libming & 4,193 & 4,856 (+15.8\%) & 51.2\% & 57.8\% (+12.9\%) \\
libxml2 & 3,765 & 4,498 (+19.5\%) & 46.3\% & 51.7\% (+11.7\%) \\
\midrule
\textbf{Average} & -- & \textbf{+17.7\%} & -- & \textbf{+12.0\%} \\
\bottomrule
\end{tabular}
\end{table}

On average, GFuzz discovers 17.7\% more unique paths and achieves 12.0\% better edge coverage than AFLGo. The improvements are consistent across all programs, with binutils and libxml2 showing the largest gains (around 19\% in path discovery).

\subsubsection{RQ2: Vulnerability Detection}

Table \ref{tab:crashes} shows the crash detection results. GFuzz demonstrates substantial improvements in finding unique crashes.

\begin{table}[t]
\centering
\caption{Crash detection comparison}
\label{tab:crashes}
\begin{tabular}{l|rr|rr}
\toprule
\textbf{Program} & \multicolumn{2}{c|}{\textbf{Unique Crashes}} & \multicolumn{2}{c}{\textbf{Time to First (min)}} \\
& AFLGo & GFuzz & AFLGo & GFuzz \\
\midrule
mJS & 18 & 24 (+33.3\%) & 42 & 28 \\
binutils & 31 & 37 (+19.4\%) & 58 & 39 \\
libming & 14 & 18 (+28.6\%) & 67 & 51 \\
libxml2 & 22 & 27 (+22.7\%) & 48 & 35 \\
\midrule
\textbf{Average} & -- & \textbf{+26.0\%} & -- & \textbf{-28.1\%} \\
\bottomrule
\end{tabular}
\end{table}

GFuzz finds 26.0\% more unique crashes on average and reduces the time to first crash by 28.1\%. The improvement in mJS is particularly notable (33.3\% more crashes), likely because the vulnerability requires specific size-related variable states that GFuzz's state diversity guidance helps explore.

\subsubsection{RQ3: Runtime Overhead}

Table \ref{tab:overhead} presents the runtime performance comparison.

\begin{table}[t]
\centering
\caption{Runtime overhead comparison (executions per second)}
\label{tab:overhead}
\begin{tabular}{l|rrr}
\toprule
\textbf{Program} & \textbf{Baseline} & \textbf{AFLGo} & \textbf{GFuzz} \\
\midrule
mJS & 1,842 & 1,654 & 1,512 \\
binutils & 2,156 & 1,923 & 1,754 \\
libming & 1,687 & 1,518 & 1,389 \\
libxml2 & 1,934 & 1,741 & 1,587 \\
\midrule
\textbf{Overhead} & -- & 10.2\% & 17.9\% \\
\bottomrule
\end{tabular}
\end{table}

GFuzz introduces an average overhead of 17.9\% compared to the uninstrumented baseline, and 8.5\% compared to AFLGo. This overhead comes primarily from the state monitoring instrumentation. Despite this overhead, GFuzz's improved effectiveness in finding vulnerabilities makes it a favorable tradeoff.

\subsubsection{RQ4: Ablation Study}

To understand the contribution of each component, we conduct an ablation study with the following configurations:

\begin{itemize}
    \item \textbf{GFuzz-nofilter}: Without semantic and memory-safety filtering (tracks all variables in distance-relevant functions)
    \item \textbf{GFuzz-noadapt}: Without adaptive weight adjustment (fixed weights)
    \item \textbf{GFuzz-full}: Complete GFuzz implementation
\end{itemize}

Table \ref{tab:ablation} shows the results on the libxml2 benchmark.

\begin{table}[t]
\centering
\caption{Ablation study on libxml2}
\label{tab:ablation}
\begin{tabular}{l|rrr}
\toprule
\textbf{Configuration} & \textbf{Paths} & \textbf{Coverage} & \textbf{Crashes} \\
\midrule
AFLGo & 3,765 & 46.3\% & 22 \\
GFuzz-nofilter & 4,012 (+6.6\%) & 49.1\% (+6.0\%) & 23 (+4.5\%) \\
GFuzz-noadapt & 4,287 (+13.9\%) & 50.4\% (+8.9\%) & 25 (+13.6\%) \\
GFuzz-full & 4,498 (+19.5\%) & 51.7\% (+11.7\%) & 27 (+22.7\%) \\
\bottomrule
\end{tabular}
\end{table}

The ablation study reveals that:
\begin{itemize}
    \item Variable filtering is essential: tracking all variables (GFuzz-nofilter) provides only marginal improvement due to overhead and noise
    \item Adaptive weight adjustment significantly enhances effectiveness: fixed weights (GFuzz-noadapt) perform substantially worse than the full GFuzz
    \item All components contribute meaningfully to the overall effectiveness
\end{itemize}

\subsection{Discussion}

Our evaluation demonstrates that GFuzz significantly outperforms AFLGo across multiple metrics while maintaining acceptable runtime overhead. The key insights are:

\begin{enumerate}
    \item \textbf{Data-state awareness matters}: Many vulnerabilities require specific variable states to trigger, and systematically exploring state diversity improves detection effectiveness.
    
    \item \textbf{Focused variable selection is crucial}: Attempting to track all variables introduces excessive overhead and noise. Our multi-strategy filtering approach identifies a focused set of relevant variables.
    
    \item \textbf{Adaptive balancing improves robustness}: Different programs and fuzzing phases benefit differently from coverage-based vs. state-based guidance. Adaptive weight adjustment allows GFuzz to emphasize whichever is currently more productive.
    
    \item \textbf{Overhead-effectiveness tradeoff is favorable}: While GFuzz introduces additional overhead, the significant improvements in vulnerability detection make it worthwhile for directed fuzzing scenarios where specific code locations are of interest.
\end{enumerate}

\section{Discussion}
\label{sec:discussion}

\subsection{When GFuzz Excels}

GFuzz is particularly effective in scenarios where:

\begin{itemize}
    \item \textbf{Data-dependent vulnerabilities}: Bugs that require specific variable values or relationships between variables (e.g., buffer overflows triggered by size mismatches)
    \item \textbf{Complex state spaces}: Programs with many variables and complex data structures where coverage-guided fuzzing struggles to explore the data dimension
    \item \textbf{Directed testing goals}: When specific code locations are of interest (patch testing, security audit, CVE reproduction)
\end{itemize}

\subsection{Limitations}

Several limitations should be noted:

\begin{enumerate}
    \item \textbf{Variable identification precision}: Our multi-strategy filtering may miss some relevant variables or include some irrelevant ones. More sophisticated static analysis (e.g., data-flow analysis, taint tracking) could improve precision.
    
    \item \textbf{Complex data structures}: Our current implementation focuses on scalar variables and strings. Complex data structures (e.g., deeply nested structures, trees, graphs) are not fully supported and could benefit from specialized handling.
    
    \item \textbf{Similarity metrics}: Our similarity metrics are relatively simple. More sophisticated metrics (e.g., learned metrics, domain-specific metrics) might improve effectiveness.
    
    \item \textbf{Overhead}: The 17.9\% overhead, while acceptable for directed fuzzing, may be prohibitive for some high-throughput scenarios.
\end{enumerate}

\subsection{Threats to Validity}

\textbf{Internal validity}: We carefully controlled experimental conditions, but fuzzing inherently involves randomness. We mitigate this with multiple trials (10 runs per configuration) and statistical analysis.

\textbf{External validity}: Our evaluation focuses on four programs in C. Generalization to other languages and domains requires further validation.

\textbf{Construct validity}: Our metrics (path discovery, coverage, crashes) are widely accepted in the fuzzing community, but they may not fully capture all aspects of fuzzing effectiveness.

\subsection{Future Work}

Several promising directions for future work include:

\begin{enumerate}
    \item \textbf{Advanced variable identification}: Incorporating more sophisticated static analysis (value-set analysis, points-to analysis) to improve variable selection
    
    \item \textbf{Structure-aware state tracking}: Extending state monitoring to handle complex data structures more effectively
    
    \item \textbf{Machine learning integration}: Using ML models to learn better similarity metrics, predict promising variable states, or optimize weight adaptation
    
    \item \textbf{Hybrid approaches}: Combining GFuzz with symbolic execution or concolic testing to handle complex constraints
    
    \item \textbf{Distributed fuzzing}: Adapting GFuzz for distributed/parallel fuzzing scenarios to scale to larger codebases
    
    \item \textbf{Language generalization}: Extending GFuzz to other languages (C++, Rust, Go) and runtime environments
\end{enumerate}

\section{Conclusion}
\label{sec:conclusion}

This paper presented GFuzz, a novel variable state diversity-guided directed fuzzing approach that enhances vulnerability detection by incorporating fine-grained data-state awareness into the fuzzing process. GFuzz systematically identifies key variables relevant to target locations, monitors their runtime states efficiently, and uses state diversity as additional feedback to guide seed selection and energy allocation. An adaptive weight adjustment mechanism dynamically balances coverage-based and state-based guidance throughout the fuzzing campaign.

Our implementation of GFuzz as an extension of AFLGo and comprehensive evaluation on four real-world programs demonstrates significant improvements: 17.7\% more path discovery, 12.0\% better code coverage, and 26.0\% more crash detection, with an acceptable 17.9\% runtime overhead. These results confirm that incorporating variable state diversity into directed fuzzing substantially enhances vulnerability detection effectiveness.

As software systems continue to grow in complexity and security importance, techniques like GFuzz that combine multiple sources of feedback (control-flow and data-state) will become increasingly important for effective automated vulnerability detection. Our work represents a step forward in this direction and opens up numerous opportunities for future research in state-aware fuzzing.

\section*{Acknowledgments}

We thank the anonymous reviewers for their valuable feedback. This work was supported by [REDACTED FOR BLIND REVIEW].

\section*{Data Availability}

The source code, benchmarks, and experimental data are available at: \url{https://github.com/cyhhtl999520/aflgo}

\begin{thebibliography}{10}

\bibitem{afl}
M.~Zalewski, ``American fuzzy lop,'' \url{http://lcamtuf.coredump.cx/afl/}, 2014.

\bibitem{libfuzzer}
LLVM~Project, ``LibFuzzer - a library for coverage-guided fuzz testing,'' \url{https://llvm.org/docs/LibFuzzer.html}, 2015.

\bibitem{aflgo}
M.~Böhme, V.-T.~Pham, M.-D.~Nguyen, and A.~Roychoudhury, ``Directed greybox fuzzing,'' in \emph{Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2017, pp. 2329--2344.

\bibitem{hawkeye}
H.~Chen, Y.~Xue, Y.~Li, B.~Chen, X.~Xie, X.~Wu, and Y.~Liu, ``Hawkeye: Towards a desired directed grey-box fuzzer,'' in \emph{Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2018, pp. 2095--2108.

\bibitem{uaflgo}
W.~Wang, J.~Xu, Y.~Zhang, and X.~Zhang, ``UAFLGo: Directed fuzzing for use-after-free vulnerabilities,'' in \emph{Proceedings of the 2020 IEEE Symposium on Security and Privacy (S\&P)}, 2020, pp. 1573--1588.

\bibitem{aflfast}
M.~Böhme, V.-T.~Pham, and A.~Roychoudhury, ``Coverage-based greybox fuzzing as markov chain,'' in \emph{Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2016, pp. 1032--1043.

\bibitem{fairfuzz}
C.~Lemieux and K.~Sen, ``FairFuzz: A targeted mutation strategy for increasing greybox fuzz testing coverage,'' in \emph{Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE)}, 2018, pp. 475--485.

\bibitem{mopt}
C.~Lyu, S.~Ji, C.~Zhang, Y.~Li, W.-H.~Lee, Y.~Song, and R.~Beyah, ``MOPT: Optimized mutation scheduling for fuzzers,'' in \emph{Proceedings of the 28th USENIX Security Symposium}, 2019, pp. 1949--1966.

\bibitem{redqueen}
C.~Aschermann, S.~Schumilo, T.~Blazytko, R.~Gawlik, and T.~Holz, ``REDQUEEN: Fuzzing with input-to-state correspondence,'' in \emph{Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS)}, 2019.

\bibitem{vuzzer}
S.~Rawat, V.~Jain, A.~Kumar, L.~Cojocar, C.~Giuffrida, and H.~Bos, ``VUzzer: Application-aware evolutionary fuzzing,'' in \emph{Proceedings of the 2017 Network and Distributed System Security Symposium (NDSS)}, 2017.

\bibitem{angora}
P.~Chen and H.~Chen, ``Angora: Efficient fuzzing by principled search,'' in \emph{Proceedings of the 2018 IEEE Symposium on Security and Privacy (S\&P)}, 2018, pp. 711--725.

\bibitem{greyone}
J.~Gan, S.~Zhang, X.~Wang, Y.~Qin, J.~Yao, and Q.~Hu, ``GREYONE: Data flow sensitive fuzzing,'' in \emph{Proceedings of the 29th USENIX Security Symposium}, 2020, pp. 2577--2594.

\bibitem{qsym}
I.~Yun, S.~Lee, M.~Xu, Y.~Jang, and T.~Kim, ``QSYM: A practical concolic execution engine tailored for hybrid fuzzing,'' in \emph{Proceedings of the 27th USENIX Security Symposium}, 2018, pp. 745--761.

\bibitem{matryoshka}
P.~Jia, P.~Cui, L.~Liu, Y.~Xie, and J.~Zhang, ``Matryoshka: Fuzzing deeply nested branches,'' in \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS)}, 2019, pp. 499--513.

\bibitem{klee}
C.~Cadar, D.~Dunbar, and D.~Engler, ``KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs,'' in \emph{Proceedings of the 8th USENIX Symposium on Operating Systems Design and Implementation (OSDI)}, 2008, pp. 209--224.

\bibitem{driller}
N.~Stephens, J.~Grosen, C.~Salls, A.~Dutcher, R.~Wang, J.~Corbetta, Y.~Shoshitaishvili, C.~Kruegel, and G.~Vigna, ``Driller: Augmenting fuzzing through selective symbolic execution,'' in \emph{Proceedings of the 2016 Network and Distributed System Security Symposium (NDSS)}, 2016.

\end{thebibliography}

\end{document}
